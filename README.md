Общая архитектура модели
Модель построена с помощью keras.Sequential и состоит из чередующихся свёрточных и подвыборочных (pooling) слоёв, за которыми следуют полносвязные слои.

Подробный разбор слоёв
Свёрточные блоки (извлечение признаков):

Conv2D(32, (3, 3), activation='relu') — первый свёрточный слой: 32 фильтра 3×3, активация ReLU.

MaxPooling2D((2, 2)) — подвыборка: уменьшает размерность в 2 раза.

Conv2D(64, (3, 3), activation='relu') — второй свёрточный слой: 64 фильтра 3×3.

MaxPooling2D((2, 2)) — ещё одна подвыборка.

Conv2D(64, (3, 3), activation='relu') — третий свёрточный слой: 64 фильтра 3×3.

Переход к полносвязной части:

Flatten() — разворачивает 2D‑карты признаков в 1D‑вектор.

Полносвязные слои (классификация):

Dense(64, activation='relu') — скрытый полносвязный слой: 64 нейрона, ReLU.

Dropout(0.5) — регуляризация: случайно обнуляет 50 % связей, чтобы предотвратить переобучение.

Dense(3, activation='softmax') — выходной слой: 3 нейрона (по числу классов), активация softmax (выдаёт вероятности для каждого класса).

Параметры модели
Вход: изображения размером 64×64 пикселя, 1 канал (оттенки серого).

Выход: вектор из 3 вероятностей (круг / квадрат / треугольник).

Функция потерь: sparse_categorical_crossentropy (так как метки — целые числа 0, 1, 2).

Оптимизатор: adam (адаптивный алгоритм оптимизации).

Метрика: точность (accuracy).

Почему именно такая архитектура?
Свёрточные слои эффективно выявляют локальные паттерны (края, углы, текстуры).

MaxPooling снижает вычислительную сложность и делает признаки более устойчивыми к сдвигам.

ReLU ускоряет обучение и помогает избежать проблемы исчезающего градиента.

Dropout уменьшает переобучение на ограниченной выборке.

Softmax на выходе даёт интерпретируемые вероятности классов.
